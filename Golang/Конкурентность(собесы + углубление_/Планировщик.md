![[Pasted image 20260126154933.png]]

![[Pasted image 20260126155011.png]]

-  [[#Зачем нам свой планировщик в GO?]]
- [[#Особенности планировщика в GO]]
- [[#Разные причины остановки]]
- [[#Устройство]]
- [[#Синхронный(блокирующий) syscall]]
- [[#Асинхронный syscall(чаще всего применяется с network i/o)]]
- [[#netpoller]]
Горутина управляется **рантаймом**, а именно его частью называемой "Планировщик". Решение о снятии горутины с выполнения принимается на основе её состояния. Решение принимает **Sysmon**

Горутина может быть вытеснена если выполняется около 10мс. И она попадает в конец глобальной очереди.

### Определения которые понадобятся здесь:

**socket** - программный интерфейс используемый для обмена данными между процессами. Комбинация IP-адреса и порта. Рано или поздно мы создадим слишком много сокетов что приведёт к ошибке `runtime'a`,  поэтому мы можем увеличить число сокетов, с помощью параметра `ulimit` в линуксе.

Каждый сокет - файловый дескриптор. В ОС на них стоит ограничение

**Файловый дескриптор (fd)** — это **номер билетик** к ресурсу в ОС. Как в парке аттракционов:

**Примеры файлов:**

```
`0 ← клавиатура (ввод) 1 ← экран (вывод)  3 ← твой файл log.txt`
```

**Сокет — тот же билетик, но к сети:**

```
`4 ← твой TCP-сокет на порту 8080`
```

`epoll` - масштабируемый интерфейс о событиях ввода/вывода(I/O). он позволяет одному потоку эффективно отслеживать состояние многих файловых дескрипторов(сокетов).

### Зачем нам свой планировщик в GO?

Чтобы не давать разрботчику потоки, а сделать вид что есть только горутины

Также, эффективное использование ресурсов процессора и кэшей.

- пул потоков - не тратится время на создание потоков.
- добавление горутин в очередь текущего процессора.
- кэши логических процессов.
- **work stealing** - логические процессоры не простаивают, а проактивно ищут себе работу.

### Особенности планировщика в GO
**Это MxN планирощик**

Количество логических процессов(**P**) изначально равно количеству ядер процессора. Регулируется параметром `GOMAXPROCS`

Планировщик реализует кооперативную многозадачность. В состоянии **waiting** горутина имееет причину остановки `WaitReason`. В зависимости от причины планировщик приннимает решение, что делать горутиной

**При работе используется минимальное количество потоков ОС.**
(например если у нас много  `io`-операций т.е.4 работы с файлами, надо ограничивать горутины иначе будет слишком много потоков ОС, а как мы знаем если потоков много то они могут сожрать всю нашу память и они и близко не так легковесны как горутины)

**Честное распределение горутин** 
Порядок получения следующей горутины:
- Проверить локальную очередь
- Проверить глобальную очередь
- Проверить сеть(netpoller)
- Work stealing(идём воровать у другого логического процессора)

###### Разные причины остановки

![[Pasted image 20260127153929.png]]

### Устройство:

GMP-model(G - goroutine, M - надстройка ОС над потоками, P - логический процессор который в себе содержит очередь горутин, кэши для стека, кэш для переменных которые остаются в куче) Кеш нужен чтобы логический процессор не ходили в общую память и не бились в мьютекс.

![[Pasted image 20260126155814.png]]

![[Pasted image 20260126160039.png]]

CPU 1 - одно ядро. OS thread - поток, грубо говоря единица кода ядру

![[Pasted image 20260126160218.png]]

Горутина main - не путать с функцией main()(составная часть этой горутины), работает изначально когда программа запускается

Как у нас запускаются горутины? 

![[Pasted image 20260126160407.png]]

Рассмотрим на примере с этой горутиной

Запустили 3 горутины они в состоянии `runnable`

![[Pasted image 20260126162237.png]]

`Wg.Wait()` сигнализирует о том что выполнение горутины останавливается и main goroutine'a останавливается, а заместо её у нас выполняется первая горутина

![[Pasted image 20260126162343.png]]

![[Pasted image 20260126162521.png]]

Потом она попадает в поток выполняется в процессоре и всё.

### Синхронный(блокирующий) syscall
**syscall** - это запрос к операционной системе (дай файл, отправь данные в интернет).

**Техническое определение:** Поток переходит в состояние `WAITING`. Планировщик ОС снимает его с выполнения на CPU. Поток буквально «замирает» и не может выполнять никакой другой код, пока ядро (Kernel) не вернет данные.

Пока у нас `G1` заблокирована в syscall'e, мы бы хотели в это время выполнять вторую и третью горутину

![[Pasted image 20260127154358.png]]

горутина уходит в kernel space.(грубо говоря пространство где Ядро, ОС, потоки и т.д). На данном примере горутина блокируется, Мы используем другой поток в ОС (**hand-off**) так как наш первый заблокировался. Поэтому запускаем другой. И грубо говоря все горутины работают уже на другом потоке, а наш первый заблокирован. Наглядная схема:

```
P1 (процессор Go) ──┬── M1 (OS-поток 1) → syscall → kernel space (блокируется)
                     │
                     └── M2 (OS-поток 2) ←── берётся из пула и продолжает работу
```

То что выполняется в kernel space - грубо говоря подкачка каких-либо данных с диска. Никак работа с процессором не связана абсолюнто

![[Pasted image 20260127154639.png]]

- **Syscall** — это запрос к операционной системе (дай файл, отправь данные в интернет).
    
- **Kernel space** — «территория» ОС, где выполняются эти запросы.
    
- **Смысл схемы:** Если один поток «уснул» на системном вызове, Go перекидывает остальные задачи на другой поток, чтобы процессор не простаивал.

### Асинхронный syscall(чаще всего применяется с network i/o)

Техническое определение: Поток делает вызов с флагом O_NONBLOCK. Если данные не готовы немедленно, ядро возвращает ошибку EAGAIN или EWOULDBLOCK. Поток не засыпает, а сразу получает управление обратно и может делать что-то другое. В Go для этого используется Network Poller (механизм на базе epoll/kqueue).

Пример на сетевом запросе:

![[Pasted image 20260127160700.png]]

Создаём сокет(быстрая операция)

Далее делаем connect и наше приложение просто идёт заниматься своими делами 

![[Pasted image 20260127160754.png]]

Можем создать много сокетов. Приложение не следит за этим. Но в какой-то момент нам нужно отправить ещё один syscall. 

Здесь грубо говоря отправляем ссылки на сокеты.

![[Pasted image 20260127160853.png]]

Сокеты меняют готовность:

![[Pasted image 20260127160922.png]]

После этого приложение может создавать запросы к этим сокетам.

![[Pasted image 20260127160954.png]]

Далее приходит ещё один `epoll_wait`. Какие то сокеты готовы к запросы, какие то к получению. Соответственно:

![[Pasted image 20260127161051.png]]

Асинхронное взаимодействие наглядно. Приложение отправляет syscall'ы в асихронном режиме. Оно не ждёт каждый сокет. ОС сама смотрит за тем у каких сокетов меняется состояние. А приложение просто получает информацию какие сокеты готовы к работе и отправляет им **syscall'ы**

| **Действие**           | **Асинхронно (твой пример с epoll)**                               | **Синхронно (Blocking)**                                              |
| ---------------------- | ------------------------------------------------------------------ | --------------------------------------------------------------------- |
| **Количество потоков** | **Один поток** может следить за 10 000 сокетов.                    | **Один поток = один сокет**. Для 10 000 сокетов нужно 10 000 потоков. |
| **Ожидание данных**    | Приложение занимается своими делами, пока ОС следит за сокетами.   | Поток «умирает» (засыпает) внутри системного вызова.                  |
| **Системные вызовы**   | Используем `epoll_wait`, чтобы получить список готовых дел пачкой. | Постоянно вызываем `read()` / `write()`, которые блокируют нас.       |
| **Роль ОС**            | ОС просто «сигналит», когда что-то готово.                         | ОС полностью управляет жизненным циклом потока (будит/усыпляет).      |

### netpoller 
- однопоточный
- использует `epoll`

G1 и G2 взаимодействуют с БДшкой(сетевой вызов). Если бы у нас были бы сетевые запросы синхронны то, у нас бы логический процессор обрабатывал бы эти горутины и дальше ничего бы не двигалось. Либо происходил бы hand-off и все горутины бы перешли в другие потоки, а потоки в свою очередь плодились бы.

![[Pasted image 20260128143420.png]]

Вместо этого горутины переходят в **netpoller(штука выполняющаяся на одном единственном потоке)**, вместо того чтобы под каждую плодить отдельный поток.

![[Pasted image 20260128143746.png]]

В стеке у нас горутины выполняются одна за другой, сначала g4 создаёт сокет, потом g1

![[Pasted image 20260128143930.png]]

**Callback queue** — это “лист ожидания” Грубо говоря. Горутины переходят в него и выполняются только когда что-то выполнится. Грубо говоря код запланированный на выполнение после события

Далее у нас сокет отправляется в ОС. В callback очередь мы закидываем, дальнейшее выполнение горутины. (она хочет отправить syscall.connect после того как сокет создастся)

![[Pasted image 20260128144009.png]]

 Потом мы делаем тоже самое с G1. G4 тем временем просто ждёт чтобы отправить syscall.connect()

![[Pasted image 20260128144159.png]]

Как только сокет подключится мы отправляем данные с горутины

![[Pasted image 20260128144321.png]]

Так как сокеты асинхронные то и g1 сразу же отправляет сокет в ОС

![[Pasted image 20260128144401.png]]

Делаем `epoll_wait` ОС говорит нам что и с какими сокетами изменилось. Callback'и отрабатывают и получаем.

![[Pasted image 20260128151042.png]]

Далее у нас что-то в callback'е ждёт получения данных, что-то отправления

![[Pasted image 20260128151256.png]]

G4 получила свои данные. То есть горутина отработала и возвращается в состояние `runnable`. 

![[Pasted image 20260128151345.png]]

Она возвращается в свой пулл.

![[Pasted image 20260128151507.png]]

у G1 пропал логический процессор в котором он выполнялся до этого. Он переносится в глобальную очередь. Потом его заберут процессоры которые работают на данный момент(CPU 2 на схеме)

![[Pasted image 20260128151530.png]]

### Long running tusk

У нас есть горутина которая работает примерно 10мс.  Если она выполняется больше sysmon тормозит её. 

![[Pasted image 20260128152527.png]]

Вытесненную таким образом горутину планировщик ставит в глобальную очередь

![[Pasted image 20260128152624.png]]

Есть ещё такая фича как `LIFO-buffer`. Допустим G2 наплодила две горутины(G4/ G5) они помещаются туда. G4 уходит в `run queue`, а G5 переходит сразу на выполнение как последняя созданная горутина. Обычно у нас горутина порождает одну асинхронно выполняющееся горутины, которую желательно выпонлить сразу при этом чтобы её не своровал другой логический процессор. То есть если мы запускали 100 горутин. Сначала выполниться сотая(**Next-P - горячая горутина**), чтобы он перешла в другой логический процессор из `run queue`

![[Pasted image 20260128152702.png]]

Проблеама в том что они обе могут бесконечно переходит из потока в `LIFO-buffer` постоянно меняя друг-друга, то как раз эти 10 мс начинают делиться между ними.  И после G2 переходит в глобальную очередь. Это называется честное распределение горутин

![[Pasted image 20260128153016.png]] 